{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsJCa3fx7YyE"
      },
      "source": [
        "# Sentiment Analysis on IMDB Reviews using LSTM\n",
        "\n",
        "\n",
        "### Steps\n",
        "<ol type=\"1\">\n",
        "    <li>Load the dataset</li>\n",
        "    <li>Clean Dataset</li>\n",
        "    <li>Encode Sentiments</li>\n",
        "    <li>Split Dataset</li>\n",
        "    <li>Tokenize and Pad/Truncate Reviews</li>\n",
        "    <li>Build Architecture/Model</li>\n",
        "    <li>Train and Test</li>\n",
        "</ol>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jjPaxzvE7YyN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd    # to load dataset\n",
        "import numpy as np     # for mathematic equation\n",
        "from nltk.corpus import stopwords   # to get collection of stopwords\n",
        "from sklearn.model_selection import train_test_split       # for splitting dataset\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer  # to encode text to int\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences   # to do padding or truncating\n",
        "from tensorflow.keras.models import Sequential     # the model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense # layers of the architecture\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint   # save model\n",
        "from tensorflow.keras.models import load_model   # load saved model\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "  Using cached tensorflow-2.17.0-cp312-cp312-win_amd64.whl.metadata (3.2 kB)\n",
            "Collecting tensorflow-intel==2.17.0 (from tensorflow)\n",
            "  Using cached tensorflow_intel-2.17.0-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\harita\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\harita\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
            "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.17.0->tensorflow)\n",
            "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting h5py>=3.10.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
            "  Downloading h5py-3.12.1-cp312-cp312-win_amd64.whl.metadata (2.5 kB)\n",
            "Collecting libclang>=13.0.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
            "Collecting ml-dtypes<0.5.0,>=0.3.1 (from tensorflow-intel==2.17.0->tensorflow)\n",
            "  Downloading ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\harita\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\harita\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\harita\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\harita\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in c:\\users\\harita\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\harita\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
            "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
            "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\harita\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.12.2)\n",
            "Collecting wrapt>=1.11.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached wrapt-1.16.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\harita\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.66.2)\n",
            "Collecting tensorboard<2.18,>=2.17 (from tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting keras>=3.2.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
            "  Downloading keras-3.6.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting numpy<2.0.0,>=1.26.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\harita\\anaconda3\\envs\\tf\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in c:\\users\\harita\\anaconda3\\envs\\tf\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.8.1)\n",
            "Collecting namex (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
            "Collecting optree (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow)\n",
            "  Downloading optree-0.13.0-cp312-cp312-win_amd64.whl.metadata (48 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\harita\\anaconda3\\envs\\tf\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\harita\\anaconda3\\envs\\tf\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\harita\\anaconda3\\envs\\tf\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\harita\\anaconda3\\envs\\tf\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.8.30)\n",
            "Collecting markdown>=2.6.8 (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow)\n",
            "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\harita\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\harita\\anaconda3\\envs\\tf\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\harita\\anaconda3\\envs\\tf\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\harita\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\harita\\anaconda3\\envs\\tf\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.2)\n",
            "Using cached tensorflow-2.17.0-cp312-cp312-win_amd64.whl (2.0 kB)\n",
            "Using cached tensorflow_intel-2.17.0-cp312-cp312-win_amd64.whl (385.2 MB)\n",
            "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
            "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Downloading h5py-3.12.1-cp312-cp312-win_amd64.whl (3.0 MB)\n",
            "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
            "   --- ------------------------------------ 0.3/3.0 MB ? eta -:--:--\n",
            "   --- ------------------------------------ 0.3/3.0 MB ? eta -:--:--\n",
            "   --- ------------------------------------ 0.3/3.0 MB ? eta -:--:--\n",
            "   --- ------------------------------------ 0.3/3.0 MB ? eta -:--:--\n",
            "   --- ------------------------------------ 0.3/3.0 MB ? eta -:--:--\n",
            "   --- ------------------------------------ 0.3/3.0 MB ? eta -:--:--\n",
            "   --- ------------------------------------ 0.3/3.0 MB ? eta -:--:--\n",
            "   ------- -------------------------------- 0.5/3.0 MB 188.5 kB/s eta 0:00:14\n",
            "   ------- -------------------------------- 0.5/3.0 MB 188.5 kB/s eta 0:00:14\n",
            "   ------- -------------------------------- 0.5/3.0 MB 188.5 kB/s eta 0:00:14\n",
            "   ------- -------------------------------- 0.5/3.0 MB 188.5 kB/s eta 0:00:14\n",
            "   ------- -------------------------------- 0.5/3.0 MB 188.5 kB/s eta 0:00:14\n",
            "   ------- -------------------------------- 0.5/3.0 MB 188.5 kB/s eta 0:00:14\n",
            "   ---------- ----------------------------- 0.8/3.0 MB 188.5 kB/s eta 0:00:12\n",
            "   ---------- ----------------------------- 0.8/3.0 MB 188.5 kB/s eta 0:00:12\n",
            "   ---------- ----------------------------- 0.8/3.0 MB 188.5 kB/s eta 0:00:12\n",
            "   ---------- ----------------------------- 0.8/3.0 MB 188.5 kB/s eta 0:00:12\n",
            "   ---------- ----------------------------- 0.8/3.0 MB 188.5 kB/s eta 0:00:12\n",
            "   ---------- ----------------------------- 0.8/3.0 MB 188.5 kB/s eta 0:00:12\n",
            "   ---------- ----------------------------- 0.8/3.0 MB 188.5 kB/s eta 0:00:12\n",
            "   ---------- ----------------------------- 0.8/3.0 MB 188.5 kB/s eta 0:00:12\n",
            "   -------------- ------------------------- 1.0/3.0 MB 173.0 kB/s eta 0:00:12\n",
            "   -------------- ------------------------- 1.0/3.0 MB 173.0 kB/s eta 0:00:12\n",
            "   -------------- ------------------------- 1.0/3.0 MB 173.0 kB/s eta 0:00:12\n",
            "   -------------- ------------------------- 1.0/3.0 MB 173.0 kB/s eta 0:00:12\n",
            "   -------------- ------------------------- 1.0/3.0 MB 173.0 kB/s eta 0:00:12\n",
            "   -------------- ------------------------- 1.0/3.0 MB 173.0 kB/s eta 0:00:12\n",
            "   -------------- ------------------------- 1.0/3.0 MB 173.0 kB/s eta 0:00:12\n",
            "   -------------- ------------------------- 1.0/3.0 MB 173.0 kB/s eta 0:00:12\n",
            "   -------------- ------------------------- 1.0/3.0 MB 173.0 kB/s eta 0:00:12\n",
            "   ----------------- ---------------------- 1.3/3.0 MB 164.1 kB/s eta 0:00:11\n",
            "   ----------------- ---------------------- 1.3/3.0 MB 164.1 kB/s eta 0:00:11\n",
            "   ----------------- ---------------------- 1.3/3.0 MB 164.1 kB/s eta 0:00:11\n",
            "   ----------------- ---------------------- 1.3/3.0 MB 164.1 kB/s eta 0:00:11\n",
            "   ----------------- ---------------------- 1.3/3.0 MB 164.1 kB/s eta 0:00:11\n",
            "   ----------------- ---------------------- 1.3/3.0 MB 164.1 kB/s eta 0:00:11\n",
            "   ----------------- ---------------------- 1.3/3.0 MB 164.1 kB/s eta 0:00:11\n",
            "   --------------------- ------------------ 1.6/3.0 MB 168.4 kB/s eta 0:00:09\n",
            "   --------------------- ------------------ 1.6/3.0 MB 168.4 kB/s eta 0:00:09\n",
            "   --------------------- ------------------ 1.6/3.0 MB 168.4 kB/s eta 0:00:09\n",
            "   --------------------- ------------------ 1.6/3.0 MB 168.4 kB/s eta 0:00:09\n",
            "   --------------------- ------------------ 1.6/3.0 MB 168.4 kB/s eta 0:00:09\n",
            "   --------------------- ------------------ 1.6/3.0 MB 168.4 kB/s eta 0:00:09\n",
            "   --------------------- ------------------ 1.6/3.0 MB 168.4 kB/s eta 0:00:09\n",
            "   ------------------------ --------------- 1.8/3.0 MB 166.4 kB/s eta 0:00:07\n",
            "   ------------------------ --------------- 1.8/3.0 MB 166.4 kB/s eta 0:00:07\n",
            "   ------------------------ --------------- 1.8/3.0 MB 166.4 kB/s eta 0:00:07\n",
            "   ------------------------ --------------- 1.8/3.0 MB 166.4 kB/s eta 0:00:07\n",
            "   ------------------------ --------------- 1.8/3.0 MB 166.4 kB/s eta 0:00:07\n",
            "   ------------------------ --------------- 1.8/3.0 MB 166.4 kB/s eta 0:00:07\n",
            "   ------------------------ --------------- 1.8/3.0 MB 166.4 kB/s eta 0:00:07\n",
            "   ------------------------ --------------- 1.8/3.0 MB 166.4 kB/s eta 0:00:07\n",
            "   ------------------------ --------------- 1.8/3.0 MB 166.4 kB/s eta 0:00:07\n",
            "   ------------------------ --------------- 1.8/3.0 MB 166.4 kB/s eta 0:00:07\n",
            "   ---------------------------- ----------- 2.1/3.0 MB 160.0 kB/s eta 0:00:06\n",
            "   ---------------------------- ----------- 2.1/3.0 MB 160.0 kB/s eta 0:00:06\n",
            "   ---------------------------- ----------- 2.1/3.0 MB 160.0 kB/s eta 0:00:06\n",
            "   ---------------------------- ----------- 2.1/3.0 MB 160.0 kB/s eta 0:00:06\n",
            "   ---------------------------- ----------- 2.1/3.0 MB 160.0 kB/s eta 0:00:06\n",
            "   ---------------------------- ----------- 2.1/3.0 MB 160.0 kB/s eta 0:00:06\n",
            "   ---------------------------- ----------- 2.1/3.0 MB 160.0 kB/s eta 0:00:06\n",
            "   ---------------------------- ----------- 2.1/3.0 MB 160.0 kB/s eta 0:00:06\n",
            "   ---------------------------- ----------- 2.1/3.0 MB 160.0 kB/s eta 0:00:06\n",
            "   ---------------------------- ----------- 2.1/3.0 MB 160.0 kB/s eta 0:00:06\n",
            "   ------------------------------- -------- 2.4/3.0 MB 154.1 kB/s eta 0:00:05\n",
            "   ------------------------------- -------- 2.4/3.0 MB 154.1 kB/s eta 0:00:05\n",
            "   ------------------------------- -------- 2.4/3.0 MB 154.1 kB/s eta 0:00:05\n",
            "   ------------------------------- -------- 2.4/3.0 MB 154.1 kB/s eta 0:00:05\n",
            "   ------------------------------- -------- 2.4/3.0 MB 154.1 kB/s eta 0:00:05\n",
            "   ------------------------------- -------- 2.4/3.0 MB 154.1 kB/s eta 0:00:05\n",
            "   ------------------------------- -------- 2.4/3.0 MB 154.1 kB/s eta 0:00:05\n",
            "   ------------------------------- -------- 2.4/3.0 MB 154.1 kB/s eta 0:00:05\n",
            "   ----------------------------------- ---- 2.6/3.0 MB 154.2 kB/s eta 0:00:03\n",
            "   ----------------------------------- ---- 2.6/3.0 MB 154.2 kB/s eta 0:00:03\n",
            "   ----------------------------------- ---- 2.6/3.0 MB 154.2 kB/s eta 0:00:03\n",
            "   ----------------------------------- ---- 2.6/3.0 MB 154.2 kB/s eta 0:00:03\n",
            "   ----------------------------------- ---- 2.6/3.0 MB 154.2 kB/s eta 0:00:03\n",
            "   ----------------------------------- ---- 2.6/3.0 MB 154.2 kB/s eta 0:00:03\n",
            "   ----------------------------------- ---- 2.6/3.0 MB 154.2 kB/s eta 0:00:03\n",
            "   ----------------------------------- ---- 2.6/3.0 MB 154.2 kB/s eta 0:00:03\n",
            "   -------------------------------------- - 2.9/3.0 MB 153.9 kB/s eta 0:00:01\n",
            "   -------------------------------------- - 2.9/3.0 MB 153.9 kB/s eta 0:00:01\n",
            "   -------------------------------------- - 2.9/3.0 MB 153.9 kB/s eta 0:00:01\n",
            "   ---------------------------------------- 3.0/3.0 MB 155.2 kB/s eta 0:00:00\n",
            "Downloading keras-3.6.0-py3-none-any.whl (1.2 MB)\n",
            "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
            "   -------- ------------------------------- 0.3/1.2 MB ? eta -:--:--\n",
            "   -------- ------------------------------- 0.3/1.2 MB ? eta -:--:--\n",
            "   -------- ------------------------------- 0.3/1.2 MB ? eta -:--:--\n",
            "   -------- ------------------------------- 0.3/1.2 MB ? eta -:--:--\n",
            "   -------- ------------------------------- 0.3/1.2 MB ? eta -:--:--\n",
            "   -------- ------------------------------- 0.3/1.2 MB ? eta -:--:--\n",
            "   -------- ------------------------------- 0.3/1.2 MB ? eta -:--:--\n",
            "   -------- ------------------------------- 0.3/1.2 MB ? eta -:--:--\n",
            "   -------- ------------------------------- 0.3/1.2 MB ? eta -:--:--\n",
            "   -------- ------------------------------- 0.3/1.2 MB ? eta -:--:--\n",
            "   -------- ------------------------------- 0.3/1.2 MB ? eta -:--:--\n",
            "   ----------------- ---------------------- 0.5/1.2 MB 122.4 kB/s eta 0:00:06\n",
            "   ----------------- ---------------------- 0.5/1.2 MB 122.4 kB/s eta 0:00:06\n",
            "   ----------------- ---------------------- 0.5/1.2 MB 122.4 kB/s eta 0:00:06\n",
            "   ----------------- ---------------------- 0.5/1.2 MB 122.4 kB/s eta 0:00:06\n",
            "   ----------------- ---------------------- 0.5/1.2 MB 122.4 kB/s eta 0:00:06\n",
            "   ----------------- ---------------------- 0.5/1.2 MB 122.4 kB/s eta 0:00:06\n",
            "   ----------------- ---------------------- 0.5/1.2 MB 122.4 kB/s eta 0:00:06\n",
            "   ----------------- ---------------------- 0.5/1.2 MB 122.4 kB/s eta 0:00:06\n",
            "   -------------------------- ------------- 0.8/1.2 MB 132.6 kB/s eta 0:00:04\n",
            "   -------------------------- ------------- 0.8/1.2 MB 132.6 kB/s eta 0:00:04\n",
            "   -------------------------- ------------- 0.8/1.2 MB 132.6 kB/s eta 0:00:04\n",
            "   -------------------------- ------------- 0.8/1.2 MB 132.6 kB/s eta 0:00:04\n",
            "   -------------------------- ------------- 0.8/1.2 MB 132.6 kB/s eta 0:00:04\n",
            "   -------------------------- ------------- 0.8/1.2 MB 132.6 kB/s eta 0:00:04\n",
            "   -------------------------- ------------- 0.8/1.2 MB 132.6 kB/s eta 0:00:04\n",
            "   -------------------------- ------------- 0.8/1.2 MB 132.6 kB/s eta 0:00:04\n",
            "   -------------------------- ------------- 0.8/1.2 MB 132.6 kB/s eta 0:00:04\n",
            "   -------------------------- ------------- 0.8/1.2 MB 132.6 kB/s eta 0:00:04\n",
            "   ----------------------------------- ---- 1.0/1.2 MB 132.1 kB/s eta 0:00:02\n",
            "   ----------------------------------- ---- 1.0/1.2 MB 132.1 kB/s eta 0:00:02\n",
            "   ----------------------------------- ---- 1.0/1.2 MB 132.1 kB/s eta 0:00:02\n",
            "   ----------------------------------- ---- 1.0/1.2 MB 132.1 kB/s eta 0:00:02\n",
            "   ----------------------------------- ---- 1.0/1.2 MB 132.1 kB/s eta 0:00:02\n",
            "   ---------------------------------------- 1.2/1.2 MB 130.9 kB/s eta 0:00:00\n",
            "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
            "Downloading ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl (127 kB)\n",
            "Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
            "Using cached tensorboard-2.17.1-py3-none-any.whl (5.5 MB)\n",
            "Downloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
            "Using cached wrapt-1.16.0-cp312-cp312-win_amd64.whl (37 kB)\n",
            "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
            "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
            "Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Downloading optree-0.13.0-cp312-cp312-win_amd64.whl (283 kB)\n",
            "Installing collected packages: namex, libclang, wrapt, termcolor, tensorboard-data-server, optree, numpy, markdown, google-pasta, gast, astunparse, tensorboard, ml-dtypes, h5py, keras, tensorflow-intel, tensorflow\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.1.1\n",
            "    Uninstalling numpy-2.1.1:\n",
            "      Successfully uninstalled numpy-2.1.1\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml_dtypes 0.5.0\n",
            "    Uninstalling ml_dtypes-0.5.0:\n",
            "      Successfully uninstalled ml_dtypes-0.5.0\n",
            "Successfully installed astunparse-1.6.3 gast-0.6.0 google-pasta-0.2.0 h5py-3.12.1 keras-3.6.0 libclang-18.1.1 markdown-3.7 ml-dtypes-0.4.1 namex-0.0.8 numpy-1.26.4 optree-0.13.0 tensorboard-2.17.1 tensorboard-data-server-0.7.2 tensorflow-2.17.0 tensorflow-intel-2.17.0 termcolor-2.5.0 wrapt-1.16.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Harita\\anaconda3\\envs\\tf\\Lib\\site-packages\\~umpy.libs'.\n",
            "  You can safely remove it manually.\n",
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Harita\\anaconda3\\envs\\tf\\Lib\\site-packages\\~umpy'.\n",
            "  You can safely remove it manually.\n"
          ]
        }
      ],
      "source": [
        "! pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "j7w08NyR7YyQ",
        "outputId": "f33ebed6-9703-405e-b4b7-ac5abca1c7ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                  review sentiment\n",
            "0      One of the other reviewers has mentioned that ...  positive\n",
            "1      A wonderful little production. <br /><br />The...  positive\n",
            "2      I thought this was a wonderful way to spend ti...  positive\n",
            "3      Basically there's a family where a little boy ...  negative\n",
            "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
            "...                                                  ...       ...\n",
            "49995  I thought this movie did a down right good job...  positive\n",
            "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
            "49997  I am a Catholic taught in parochial elementary...  negative\n",
            "49998  I'm going to have to disagree with the previou...  negative\n",
            "49999  No one expects the Star Trek movies to be high...  negative\n",
            "\n",
            "[50000 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('IMDB Dataset.csv')\n",
        "\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xICDsFvW7YyS"
      },
      "outputs": [],
      "source": [
        "english_stops = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GpGKCrF47YyU",
        "outputId": "35884ea4-162d-451b-eaad-b50fb8d4bb3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reviews\n",
            "0        [one, reviewers, mentioned, watching, oz, epis...\n",
            "1        [a, wonderful, little, production, the, filmin...\n",
            "2        [i, thought, wonderful, way, spend, time, hot,...\n",
            "3        [basically, family, little, boy, jake, thinks,...\n",
            "4        [petter, mattei, love, time, money, visually, ...\n",
            "                               ...                        \n",
            "49995    [i, thought, movie, right, good, job, it, crea...\n",
            "49996    [bad, plot, bad, dialogue, bad, acting, idioti...\n",
            "49997    [i, catholic, taught, parochial, elementary, s...\n",
            "49998    [i, going, disagree, previous, comment, side, ...\n",
            "49999    [no, one, expects, star, trek, movies, high, a...\n",
            "Name: review, Length: 50000, dtype: object \n",
            "\n",
            "Sentiment\n",
            "0        1\n",
            "1        1\n",
            "2        1\n",
            "3        0\n",
            "4        1\n",
            "        ..\n",
            "49995    1\n",
            "49996    0\n",
            "49997    0\n",
            "49998    0\n",
            "49999    0\n",
            "Name: sentiment, Length: 50000, dtype: int64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Harita\\AppData\\Local\\Temp\\ipykernel_95024\\4067306172.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  y_data = y_data.replace('negative', 0)\n"
          ]
        }
      ],
      "source": [
        "def load_dataset():\n",
        "    df = pd.read_csv('IMDB Dataset.csv')\n",
        "    x_data = df['review']       # Reviews/Input\n",
        "    y_data = df['sentiment']    # Sentiment/Output\n",
        "\n",
        "    # PRE-PROCESS REVIEW\n",
        "    x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n",
        "    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n",
        "    x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])  # remove stop words\n",
        "    x_data = x_data.apply(lambda review: [w.lower() for w in review])   # lower case\n",
        "\n",
        "    # ENCODE SENTIMENT -> 0 & 1\n",
        "    y_data = y_data.replace('positive', 1)\n",
        "    y_data = y_data.replace('negative', 0)\n",
        "\n",
        "    return x_data, y_data\n",
        "\n",
        "x_data, y_data = load_dataset()\n",
        "\n",
        "print('Reviews')\n",
        "print(x_data, '\\n')\n",
        "print('Sentiment')\n",
        "print(y_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6p9Q6zy57YyV",
        "outputId": "da414ea3-84fd-4353-9eef-6f94468df28b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Set\n",
            "39941    [this, result, town, milpitas, california, mak...\n",
            "21733    [i, agree, previous, reviewer, this, film, awe...\n",
            "49746    [had, to, kill, a, mockingbird, movie, would, ...\n",
            "26291    [despite, negative, criticism, i, really, enjo...\n",
            "9311     [people, criticize, nsna, low, point, bond, go...\n",
            "                               ...                        \n",
            "42562    [this, enjoyable, minor, noir, boasts, top, ca...\n",
            "38827    [this, moody, creepy, horror, flick, begins, c...\n",
            "9134     [basing, television, series, popular, author, ...\n",
            "1519     [an, ear, splitting, movie, quasi, old, fashio...\n",
            "7        [this, show, amazing, fresh, innovative, idea,...\n",
            "Name: review, Length: 40000, dtype: object \n",
            "\n",
            "22387    [dr, paul, flanner, richard, gere, successful,...\n",
            "31166    [after, watching, half, i, ready, give, turn, ...\n",
            "34395    [i, loved, film, audience, i, part, loved, fil...\n",
            "31079    [i, want, clarify, things, i, familiar, ming, ...\n",
            "14725    [a, stunning, piece, art, you, watch, every, i...\n",
            "                               ...                        \n",
            "24748    [this, well, done, action, movie, there, plent...\n",
            "9004     [the, best, thing, one, say, film, traffic, br...\n",
            "7156     [this, movie, terrible, i, rented, knowing, ex...\n",
            "48511    [this, typical, cheerful, colorful, mgm, music...\n",
            "336      [elvira, mistress, dark, one, fav, movies, eve...\n",
            "Name: review, Length: 10000, dtype: object \n",
            "\n",
            "Test Set\n",
            "39941    0\n",
            "21733    0\n",
            "49746    1\n",
            "26291    1\n",
            "9311     1\n",
            "        ..\n",
            "42562    1\n",
            "38827    1\n",
            "9134     0\n",
            "1519     0\n",
            "7        0\n",
            "Name: sentiment, Length: 40000, dtype: int64 \n",
            "\n",
            "22387    1\n",
            "31166    0\n",
            "34395    1\n",
            "31079    0\n",
            "14725    1\n",
            "        ..\n",
            "24748    1\n",
            "9004     1\n",
            "7156     0\n",
            "48511    1\n",
            "336      1\n",
            "Name: sentiment, Length: 10000, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2)\n",
        "\n",
        "print('Train Set')\n",
        "print(x_train, '\\n')\n",
        "print(x_test, '\\n')\n",
        "print('Test Set')\n",
        "print(y_train, '\\n')\n",
        "print(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zmX0apie7YyW"
      },
      "outputs": [],
      "source": [
        "def get_max_length():\n",
        "    review_length = []\n",
        "    for review in x_train:\n",
        "        review_length.append(len(review))\n",
        "\n",
        "    return int(np.ceil(np.mean(review_length)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IHeMk1y-7YyW",
        "outputId": "d82c5fa6-5124-47a6-edd7-8d677fd4198c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoded X Train\n",
            " [[    8   864   397 ...     0     0     0]\n",
            " [    1   921   844 ...     0     0     0]\n",
            " [ 2912   275   401 ...    58    76  1099]\n",
            " ...\n",
            " [18691   595   109 ...     0     0     0]\n",
            " [  698  4626  8109 ...     0     0     0]\n",
            " [    8    46   384 ...     0     0     0]] \n",
            "\n",
            "Encoded X Test\n",
            " [[  677   725 25067 ...     0     0     0]\n",
            " [  305    66   210 ...     0     0     0]\n",
            " [    1   343     4 ...     0     0     0]\n",
            " ...\n",
            " [    8     3   284 ...     0     0     0]\n",
            " [    8   689  7723 ...   104   531   158]\n",
            " [ 4735  4359   358 ...     0     0     0]] \n",
            "\n",
            "Maximum review length:  130\n"
          ]
        }
      ],
      "source": [
        "# ENCODE REVIEW\n",
        "token = Tokenizer(lower=False)    # no need lower, because already lowered the data in load_data()\n",
        "token.fit_on_texts(x_train)\n",
        "x_train = token.texts_to_sequences(x_train)\n",
        "x_test = token.texts_to_sequences(x_test)\n",
        "\n",
        "max_length = get_max_length()\n",
        "\n",
        "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
        "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "total_words = len(token.word_index) + 1   # add 1 because of 0 padding\n",
        "\n",
        "print('Encoded X Train\\n', x_train, '\\n')\n",
        "print('Encoded X Test\\n', x_test, '\\n')\n",
        "print('Maximum review length: ', max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "39d9iw587YyX",
        "outputId": "fdf13bdc-ca15-42dd-88ea-45b34aefc86e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Harita\\anaconda3\\envs\\tf\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "# ARCHITECTURE\n",
        "EMBED_DIM = 32\n",
        "LSTM_OUT = 64\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, EMBED_DIM, input_length = max_length))\n",
        "model.add(LSTM(LSTM_OUT))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtA4iq9u7YyY"
      },
      "outputs": [],
      "source": [
        "checkpoint = ModelCheckpoint(\n",
        "    'models/LSTM.h5',\n",
        "    monitor='accuracy',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6RwgufF7YyY",
        "outputId": "f2135481-a44d-4056-929d-e56e1e16985c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train on 40000 samples\n",
            "Epoch 1/5\n",
            "39808/40000 [============================>.] - ETA: 0s - loss: 0.5096 - accuracy: 0.7128\n",
            "Epoch 00001: accuracy improved from -inf to 0.71360, saving model to models/LSTM.h5\n",
            "40000/40000 [==============================] - 13s 327us/sample - loss: 0.5087 - accuracy: 0.7136\n",
            "Epoch 2/5\n",
            "39808/40000 [============================>.] - ETA: 0s - loss: 0.2301 - accuracy: 0.9157\n",
            "Epoch 00002: accuracy improved from 0.71360 to 0.91570, saving model to models/LSTM.h5\n",
            "40000/40000 [==============================] - 10s 257us/sample - loss: 0.2302 - accuracy: 0.9157\n",
            "Epoch 3/5\n",
            "39808/40000 [============================>.] - ETA: 0s - loss: 0.1344 - accuracy: 0.9584\n",
            "Epoch 00003: accuracy improved from 0.91570 to 0.95837, saving model to models/LSTM.h5\n",
            "40000/40000 [==============================] - 11s 280us/sample - loss: 0.1345 - accuracy: 0.9584\n",
            "Epoch 4/5\n",
            "39808/40000 [============================>.] - ETA: 0s - loss: 0.0848 - accuracy: 0.9762\n",
            "Epoch 00004: accuracy improved from 0.95837 to 0.97620, saving model to models/LSTM.h5\n",
            "40000/40000 [==============================] - 10s 254us/sample - loss: 0.0848 - accuracy: 0.9762\n",
            "Epoch 5/5\n",
            "39808/40000 [============================>.] - ETA: 0s - loss: 0.0595 - accuracy: 0.9848\n",
            "Epoch 00005: accuracy improved from 0.97620 to 0.98488, saving model to models/LSTM.h5\n",
            "40000/40000 [==============================] - 10s 259us/sample - loss: 0.0593 - accuracy: 0.9849\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x1b130e98b08>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(x_train, y_train, batch_size = 128, epochs = 5, callbacks=[checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ik8g11ZO7YyZ",
        "outputId": "68e6b27c-750b-4063-fe73-7e4b18f519d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correct Prediction: 8609\n",
            "Wrong Prediction: 1391\n",
            "Accuracy: 86.09\n"
          ]
        }
      ],
      "source": [
        "y_pred = model.predict_classes(x_test, batch_size = 128)\n",
        "\n",
        "true = 0\n",
        "for i, y in enumerate(y_test):\n",
        "    if y == y_pred[i]:\n",
        "        true += 1\n",
        "\n",
        "print('Correct Prediction: {}'.format(true))\n",
        "print('Wrong Prediction: {}'.format(len(y_pred) - true))\n",
        "print('Accuracy: {}'.format(true/len(y_pred)*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlSf6-h67Yya"
      },
      "outputs": [],
      "source": [
        "loaded_model = load_model('models/LSTM.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5cS4_Ty7Yyb",
        "outputId": "7d745dc1-160b-4faf-e3ba-fae8a95d153a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Movie Review: Nothing was typical about this. Everything was beautifully done in this movie, the story, the flow, the scenario, everything. I highly recommend it for mystery lovers, for anyone who wants to watch a good movie!\n"
          ]
        }
      ],
      "source": [
        "review = str(input('Movie Review: '))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhzbyPen7Yyb",
        "outputId": "44198017-a063-4724-ad3e-c7a666b5ce42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned:  Nothing was typical about this Everything was beautifully done in this movie the story the flow the scenario everything I highly recommend it for mystery lovers for anyone who wants to watch a good movie\n",
            "Filtered:  ['nothing typical everything beautifully done movie story flow scenario everything i highly recommend mystery lovers anyone wants watch good movie']\n"
          ]
        }
      ],
      "source": [
        "# Pre-process input\n",
        "regex = re.compile(r'[^a-zA-Z\\s]')\n",
        "review = regex.sub('', review)\n",
        "print('Cleaned: ', review)\n",
        "\n",
        "words = review.split(' ')\n",
        "filtered = [w for w in words if w not in english_stops]\n",
        "filtered = ' '.join(filtered)\n",
        "filtered = [filtered.lower()]\n",
        "\n",
        "print('Filtered: ', filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWEzMScf7Yyc",
        "outputId": "203d95b9-0c80-4113-e9f5-2d6d1ca08b15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[  76  708  173 1155  127    3   13 2751 2656  173    1  449  281  701\n",
            "  1710  153  401   33    9    3    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0]]\n"
          ]
        }
      ],
      "source": [
        "tokenize_words = token.texts_to_sequences(filtered)\n",
        "tokenize_words = pad_sequences(tokenize_words, maxlen=max_length, padding='post', truncating='post')\n",
        "print(tokenize_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bNzI0WZ7Yyd",
        "outputId": "f78da82e-ef07-441a-971f-cbf7f454599d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.9971666]]\n"
          ]
        }
      ],
      "source": [
        "result = loaded_model.predict(tokenize_words)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jy_P0eyB7Yye",
        "outputId": "1b0c929d-e333-4e5b-d947-5029d1ef22af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "positive\n"
          ]
        }
      ],
      "source": [
        "if result >= 0.7:\n",
        "    print('positive')\n",
        "else:\n",
        "    print('negative')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
